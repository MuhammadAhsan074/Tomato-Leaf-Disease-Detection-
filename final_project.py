# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13s1pnCllxFwgpktNTBGJr15ZB4v7dRZy
"""



# ============================================================
# PROFESSIONAqL TOMATO LEAF DISEASE CLASSIFICATION (v2.0)
# FEATURES: MobileNetV2 | 95% Target | Advanced Visualization
# ============================================================

# ----------------------------
# 1. IMPORT LIBRARIES
# ----------------------------
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
import pickle  # Added for .pkl saving
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import os

# Set professional plotting style
sns.set_theme(style="whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)

# ----------------------------
# 2. MOUNT GOOGLE DRIVE
# ----------------------------
from google.colab import drive
drive.mount('/content/drive')

# ----------------------------
# 3. DATASET PATH
# ----------------------------
DATASET_PATH = "/content/drive/MyDrive/Tomato datasets"
if os.path.exists(DATASET_PATH):
    print("Dataset found. Folders:", os.listdir(DATASET_PATH))
else:
    print(f"ERROR: Path {DATASET_PATH} not found. Please check your Drive path.")

# ----------------------------
# 4. PARAMETERS
# ----------------------------
IMG_SIZE = (224, 224)
BATCH_SIZE = 32
SEED = 42
EPOCHS = 40

# ----------------------------
# 5. LOAD DATASET
# ----------------------------
train_ds = tf.keras.utils.image_dataset_from_directory(
    DATASET_PATH,
    validation_split=0.2,
    subset="training",
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    DATASET_PATH,
    validation_split=0.2,
    subset="validation",
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

class_names = train_ds.class_names
num_classes = len(class_names)
print("Classes detected:", class_names)

# ----------------------------
# 6. PREVENT IMAGE ERRORS & OPTIMIZE
# ----------------------------
# Using ignore_errors to skip corrupt images during pipeline
train_ds = train_ds.apply(tf.data.experimental.ignore_errors())
val_ds = val_ds.apply(tf.data.experimental.ignore_errors())

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1500).prefetch(AUTOTUNE)
val_ds = val_ds.cache().prefetch(AUTOTUNE)

# ----------------------------
# 7. VISUALIZATION 1: CLASS DISTRIBUTION
# ----------------------------
# Note: Iterating to count requires one pass through dataset
print("Computing class distribution (this may take a moment)...")
train_labels = []
for _, y in train_ds:
    train_labels.extend(y.numpy())

plt.figure(figsize=(10, 5))
sns.countplot(x=train_labels)
plt.xticks(ticks=range(num_classes), labels=class_names, rotation=45)
plt.title("Graph 1: Training Data Class Distribution")
plt.xlabel("Class Name")
plt.ylabel("Number of Images")
plt.show()

# ----------------------------
# 8. STRONG DATA AUGMENTATION
# ----------------------------
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.25),
    layers.RandomZoom(0.3),
    layers.RandomContrast(0.3),
    layers.RandomBrightness(0.2), # Added brightness for robustness
])

# ----------------------------
# 9. LOAD PRETRAINED MODEL
# ----------------------------
base_model = tf.keras.applications.MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,
    weights="imagenet"
)

# Fine-tuning: freeze lower layers
base_model.trainable = True
for layer in base_model.layers[:-50]:
    layer.trainable = False

# ----------------------------
# 10. BUILD MODEL
# ----------------------------
model = models.Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.BatchNormalization(),
    layers.Dense(256, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation="softmax")
])

# ----------------------------
# 11. COMPILE MODEL
# ----------------------------
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

# ----------------------------
# 12. COMPUTE CLASS WEIGHTS
# ----------------------------
# Balancing weights helps if one disease has fewer photos than others
class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(train_labels),
    y=train_labels
)
class_weights = dict(enumerate(class_weights))
print("Class weights:", class_weights)

# ----------------------------
# 13. CALLBACKS
# ----------------------------
callbacks = [
    tf.keras.callbacks.EarlyStopping(
        patience=8,
        restore_best_weights=True,
        monitor='val_loss'
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        factor=0.3,
        patience=4,
        min_lr=1e-7,
        monitor='val_loss'
    )
]

# ----------------------------
# 14. TRAIN MODEL
# ----------------------------
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    class_weight=class_weights,
    callbacks=callbacks
)

# ============================================================
# NEW: EXTENDED VISUALIZATION SECTION (5 GRAPHS TOTAL)
# ============================================================

# ----------------------------
# Graph 2 & 3: Accuracy and Loss
# ----------------------------
plt.figure(figsize=(15, 6))

# Accuracy Plot
plt.subplot(1, 2, 1)
plt.plot(history.history["accuracy"], label="Train Accuracy", linewidth=2)
plt.plot(history.history["val_accuracy"], label="Val Accuracy", linewidth=2)
plt.title("Graph 2: Accuracy Over Time")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

# Loss Plot
plt.subplot(1, 2, 2)
plt.plot(history.history["loss"], label="Train Loss", linewidth=2, color='orange')
plt.plot(history.history["val_loss"], label="Val Loss", linewidth=2, color='red')
plt.title("Graph 3: Loss Over Time")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

# ----------------------------
# Graph 4: Learning Rate Decay
# ----------------------------
plt.figure(figsize=(10, 4))
plt.plot(history.history["learning_rate"], marker='o', linestyle='--', color='purple')
plt.title("Graph 4: Learning Rate Adjustment Schedule")
plt.xlabel("Epochs")
plt.ylabel("Learning Rate")
plt.yscale("log") # Log scale makes small changes visible
plt.grid(True, which="both", ls="-", alpha=0.5)
plt.show()

# ----------------------------
# EVALUATION & PREDICTIONS
# ----------------------------
print("Generating predictions for evaluation...")
y_true = []
y_pred = []

# Iterate over validation set to get true labels and predictions
for images, labels in val_ds:
    preds = model.predict(images, verbose=0)
    y_true.extend(labels.numpy())
    y_pred.extend(np.argmax(preds, axis=1))

# ----------------------------
# Graph 5: Confusion Matrix
# ----------------------------
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(
    cm, annot=True, fmt="d",
    cmap="Blues",
    xticklabels=class_names,
    yticklabels=class_names
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Graph 5: Confusion Matrix")
plt.show()

# ----------------------------
# Graph 6: Per-Class Performance (Bonus)
# ----------------------------
report_dict = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)
df_report = pd.DataFrame(report_dict).transpose()
df_report = df_report.iloc[:-3, :] # Exclude 'accuracy', 'macro avg', 'weighted avg'

plt.figure(figsize=(10, 6))
sns.barplot(x=df_report.index, y=df_report['f1-score'], palette="viridis", hue=df_report.index)
plt.title("Graph 6: F1-Score per Class (Precision/Recall Balance)")
plt.ylim(0, 1.0)
plt.ylabel("F1 Score")
plt.show()

print("\n=== CLASSIFICATION REPORT ===")
print(classification_report(y_true, y_pred, target_names=class_names))

# ============================================================
# NEW: DATASET EXAMPLES (3 IMAGES PER CLASS)
# ============================================================

def plot_examples_per_class(dataset, class_names, samples_per_class=3):
    print("\nExtracting example images for gallery...")
    # Create a dictionary to hold images: { 'Early_blight': [], 'healthy': [] ... }
    example_store = {name: [] for name in class_names}

    # Iterate through the dataset until we have enough samples
    for images, labels in dataset:
        for i in range(len(labels)):
            label_idx = labels[i].numpy()
            class_name = class_names[label_idx]

            # If we need more images for this class, add it
            if len(example_store[class_name]) < samples_per_class:
                # Convert tensor to numpy and rescale for display if needed
                img = images[i].numpy().astype("uint8")
                example_store[class_name].append(img)

        # Check if we have enough for all classes
        all_full = all(len(v) >= samples_per_class for v in example_store.values())
        if all_full:
            break

    # Plotting
    cols = samples_per_class
    rows = len(class_names)
    fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*4))
    fig.suptitle(f"Dataset Examples ({samples_per_class} per class)", fontsize=16)

    for row_idx, class_name in enumerate(class_names):
        imgs = example_store[class_name]
        for col_idx in range(cols):
            ax = axes[row_idx, col_idx] if rows > 1 else axes[col_idx]
            if col_idx < len(imgs):
                ax.imshow(imgs[col_idx])
                ax.set_title(class_name)
                ax.axis("off")
            else:
                ax.axis("off") # Hide axis if image missing

    plt.tight_layout()
    plt.subplots_adjust(top=0.95) # Make room for title
    plt.show()

# Run the gallery function
plot_examples_per_class(val_ds.unbatch().batch(32), class_names, samples_per_class=3)

# ----------------------------
# SAVE MODEL
# ----------------------------
# 1. Save in native Keras format (Recommended)
model.save("/content/drive/MyDrive/tomato_leaf_disease_model_professional_v2.keras")
print("Model saved successfully as .keras")

# 2. Save in Pickle format (Requested)
pkl_path = "/content/drive/MyDrive/tomato_leaf_disease_model.pkl"
with open(pkl_path, 'wb') as f:
    pickle.dump(model, f)
print(f"Model saved successfully as .pkl at {pkl_path}")

